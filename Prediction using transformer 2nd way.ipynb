{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e2dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "#import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "#from icecream import ic\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "#from model import Transformer\n",
    "import torch.nn as nn\n",
    "#from DataLoader import SensorDataset\n",
    "import logging\n",
    "#from plot import *\n",
    "#from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda3a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "Dataset = pd.read_csv(\"dataset_tk.csv\")\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_name, root_dir, training_length, forecast_window):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            root_dir (string): Directory\n",
    "        \"\"\"\n",
    "        \n",
    "        # load raw data file\n",
    "        csv_file = os.path.join(root_dir, csv_name)\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = MinMaxScaler()\n",
    "        self.T = training_length\n",
    "        self.S = forecast_window\n",
    "\n",
    "    def __len__(self):\n",
    "        # return number of sensors\n",
    "        return len(self.df.groupby(by=[\"reindexed_id\"]))\n",
    "\n",
    "    # Will pull an index between 0 and __len__. \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Sensors are indexed from 1\n",
    "        idx = idx+1\n",
    "\n",
    "        # np.random.seed(0)\n",
    "\n",
    "        start = np.random.randint(0, len(self.df[self.df[\"reindexed_id\"]==idx]) - self.T - self.S) \n",
    "        sensor_number = str(self.df[self.df[\"reindexed_id\"]==idx][[\"sensor_id\"]][start:start+1].values.item())\n",
    "        index_in = torch.tensor([i for i in range(start, start+self.T)])\n",
    "        index_tar = torch.tensor([i for i in range(start + self.T, start + self.T + self.S)])\n",
    "        _input = torch.tensor(self.df[self.df[\"reindexed_id\"]==idx][[\"humidity\", \"sin_hour\", \"cos_hour\", \"sin_day\", \"cos_day\", \"sin_month\", \"cos_month\"]][start : start + self.T].values)\n",
    "        target = torch.tensor(self.df[self.df[\"reindexed_id\"]==idx][[\"humidity\", \"sin_hour\", \"cos_hour\", \"sin_day\", \"cos_day\", \"sin_month\", \"cos_month\"]][start + self.T : start + self.T + self.S].values)\n",
    "\n",
    "        # scalar is fit only to the input, to avoid the scaled values \"leaking\" information about the target range.\n",
    "        # scalar is fit only for humidity, as the timestamps are already scaled\n",
    "        # scalar input/output of shape: [n_samples, n_features].\n",
    "        scaler = self.transform\n",
    "\n",
    "        scaler.fit(_input[:,0].unsqueeze(-1))\n",
    "        _input[:,0] = torch.tensor(scaler.transform(_input[:,0].unsqueeze(-1)).squeeze(-1))\n",
    "        target[:,0] = torch.tensor(scaler.transform(target[:,0].unsqueeze(-1)).squeeze(-1))\n",
    "\n",
    "        # save the scalar to be used later when inverse translating the data for plotting.\n",
    "        dump(scaler, 'scalar_item.joblib')\n",
    "\n",
    "        return index_in, index_tar, _input, target, sensor_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbddb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the timestamp data cyclically. See Medium Article.\n",
    "def process_data(source):\n",
    "\n",
    "    df = pd.read_csv(source)\n",
    "        \n",
    "    timestamps = [ts.split('+')[0] for ts in  df['timestamp']]\n",
    "    timestamps_hour = np.array([float(datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S').hour) for t in timestamps])\n",
    "    timestamps_day = np.array([float(datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S').day) for t in timestamps])\n",
    "    timestamps_month = np.array([float(datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S').month) for t in timestamps])\n",
    "\n",
    "    hours_in_day = 24\n",
    "    days_in_month = 30\n",
    "    month_in_year = 12\n",
    "\n",
    "    df['sin_hour'] = np.sin(2*np.pi*timestamps_hour/hours_in_day)\n",
    "    df['cos_hour'] = np.cos(2*np.pi*timestamps_hour/hours_in_day)\n",
    "    df['sin_day'] = np.sin(2*np.pi*timestamps_day/days_in_month)\n",
    "    df['cos_day'] = np.cos(2*np.pi*timestamps_day/days_in_month)\n",
    "    df['sin_month'] = np.sin(2*np.pi*timestamps_month/month_in_year)\n",
    "    df['cos_month'] = np.cos(2*np.pi*timestamps_month/month_in_year)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_dataset = process_data('Data/train_raw.csv')\n",
    "test_dataset = process_data('Data/test_raw.csv')\n",
    "\n",
    "train_dataset.to_csv(r'Data/train_dataset.csv', index=False)\n",
    "test_dataset.to_csv(r'Data/test_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# save train or validation loss\n",
    "def log_loss(loss_val : float, path_to_save_loss : str, train : bool = True):\n",
    "    if train:\n",
    "        file_name = \"train_loss.txt\"\n",
    "    else:\n",
    "        file_name = \"val_loss.txt\"\n",
    "\n",
    "    path_to_file = path_to_save_loss+file_name\n",
    "    os.makedirs(os.path.dirname(path_to_file), exist_ok=True)\n",
    "    with open(path_to_file, \"a\") as f:\n",
    "        f.write(str(loss_val)+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "# Exponential Moving Average, https://en.wikipedia.org/wiki/Moving_average\n",
    "def EMA(values, alpha=0.1):\n",
    "    ema_values = [values[0]]\n",
    "    for idx, item in enumerate(values[1:]):\n",
    "        ema_values.append(alpha*item + (1-alpha)*ema_values[idx])\n",
    "    return ema_values\n",
    "\n",
    "# Remove all files from previous executions and re-run the model.\n",
    "def clean_directory():\n",
    "\n",
    "    if os.path.exists('save_loss'):\n",
    "        shutil.rmtree('save_loss')\n",
    "    if os.path.exists('save_model'): \n",
    "        shutil.rmtree('save_model')\n",
    "    if os.path.exists('save_predictions'): \n",
    "        shutil.rmtree('save_predictions')\n",
    "    os.mkdir(\"save_loss\")\n",
    "    os.mkdir(\"save_model\")\n",
    "    os.mkdir(\"save_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s %(message)s\", datefmt=\"[%Y-%m-%d %H:%M:%S]\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def inference(path_to_save_predictions, forecast_window, dataloader, device, path_to_save_model, best_model):\n",
    "\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    model = Transformer().double().to(device)\n",
    "    model.load_state_dict(torch.load(path_to_save_model+best_model))\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        for plot in range(25):\n",
    "\n",
    "            for index_in, index_tar, _input, target, sensor_number in dataloader:\n",
    "                \n",
    "                # starting from 1 so that src matches with target, but has same length as when training\n",
    "                src = _input.permute(1,0,2).double().to(device)[1:, :, :] # 47, 1, 7: t1 -- t47\n",
    "                target = target.permute(1,0,2).double().to(device) # t48 - t59\n",
    "\n",
    "                next_input_model = src\n",
    "                all_predictions = []\n",
    "\n",
    "                for i in range(forecast_window - 1):\n",
    "                    \n",
    "                    prediction = model(next_input_model, device) # 47,1,1: t2' - t48'\n",
    "\n",
    "                    if all_predictions == []:\n",
    "                        all_predictions = prediction # 47,1,1: t2' - t48'\n",
    "                    else:\n",
    "                        all_predictions = torch.cat((all_predictions, prediction[-1,:,:].unsqueeze(0))) # 47+,1,1: t2' - t48', t49', t50'\n",
    "\n",
    "                    pos_encoding_old_vals = src[i+1:, :, 1:] # 46, 1, 6, pop positional encoding first value: t2 -- t47\n",
    "                    pos_encoding_new_val = target[i + 1, :, 1:].unsqueeze(1) # 1, 1, 6, append positional encoding of last predicted value: t48\n",
    "                    pos_encodings = torch.cat((pos_encoding_old_vals, pos_encoding_new_val)) # 47, 1, 6 positional encodings matched with prediction: t2 -- t48\n",
    "                    \n",
    "                    next_input_model = torch.cat((src[i+1:, :, 0].unsqueeze(-1), prediction[-1,:,:].unsqueeze(0))) #t2 -- t47, t48'\n",
    "                    next_input_model = torch.cat((next_input_model, pos_encodings), dim = 2) # 47, 1, 7 input for next round\n",
    "\n",
    "                true = torch.cat((src[1:,:,0],target[:-1,:,0]))\n",
    "                loss = criterion(true, all_predictions[:,:,0])\n",
    "                val_loss += loss\n",
    "            \n",
    "            val_loss = val_loss/10\n",
    "            scaler = load('scalar_item.joblib')\n",
    "            src_humidity = scaler.inverse_transform(src[:,:,0].cpu())\n",
    "            target_humidity = scaler.inverse_transform(target[:,:,0].cpu())\n",
    "            prediction_humidity = scaler.inverse_transform(all_predictions[:,:,0].detach().cpu().numpy())\n",
    "            plot_prediction(plot, path_to_save_predictions, src_humidity, target_humidity, prediction_humidity, sensor_number, index_in, index_tar)\n",
    "\n",
    "        logger.info(f\"Loss On Unseen Dataset: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# from train_teacher_forcing import *\n",
    "from train_with_sampling import *\n",
    "from DataLoader import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from helpers import *\n",
    "from inference import *\n",
    "\n",
    "def main(\n",
    "    epoch: int = 1000,\n",
    "    k: int = 60,\n",
    "    batch_size: int = 1,\n",
    "    frequency: int = 100,\n",
    "    training_length = 48,\n",
    "    forecast_window = 24,\n",
    "    train_csv = \"train_dataset.csv\",\n",
    "    test_csv = \"test_dataset.csv\",\n",
    "    path_to_save_model = \"save_model/\",\n",
    "    path_to_save_loss = \"save_loss/\", \n",
    "    path_to_save_predictions = \"save_predictions/\", \n",
    "    device = \"cpu\"\n",
    "):\n",
    "\n",
    "    clean_directory()\n",
    "\n",
    "    train_dataset = SensorDataset(csv_name = train_csv, root_dir = \"Data/\", training_length = training_length, forecast_window = forecast_window)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    test_dataset = SensorDataset(csv_name = test_csv, root_dir = \"Data/\", training_length = training_length, forecast_window = forecast_window)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    best_model = transformer(train_dataloader, epoch, k, frequency, path_to_save_model, path_to_save_loss, path_to_save_predictions, device)\n",
    "    inference(path_to_save_predictions, forecast_window, test_dataloader, device, path_to_save_model, best_model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epoch\", type=int, default=1000)\n",
    "    parser.add_argument(\"--k\", type=int, default=60)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--frequency\", type=int, default=100)\n",
    "    parser.add_argument(\"--path_to_save_model\",type=str,default=\"save_model/\")\n",
    "    parser.add_argument(\"--path_to_save_loss\",type=str,default=\"save_loss/\")\n",
    "    parser.add_argument(\"--path_to_save_predictions\",type=str,default=\"save_predictions/\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(\n",
    "        epoch=args.epoch,\n",
    "        k = args.k,\n",
    "        batch_size=args.batch_size,\n",
    "        frequency=args.frequency,\n",
    "        path_to_save_model=args.path_to_save_model,\n",
    "        path_to_save_loss=args.path_to_save_loss,\n",
    "        path_to_save_predictions=args.path_to_save_predictions,\n",
    "        device=args.device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch, math\n",
    "from icecream import ic\n",
    "import time\n",
    "\"\"\"\n",
    "The architecture is based on the paper “Attention Is All You Need”. \n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    # d_model : number of features\n",
    "    def __init__(self,feature_size=7,num_layers=3,dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=7, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, device):\n",
    "        \n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src,mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949501a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from helpers import EMA\n",
    "from icecream import ic \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_loss(path_to_save, train=True):\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    with open(path_to_save + \"/train_loss.txt\", 'r') as f:\n",
    "        loss_list = [float(line) for line in f.readlines()]\n",
    "    if train:\n",
    "        title = \"Train\"\n",
    "    else:\n",
    "        title = \"Validation\"\n",
    "    EMA_loss = EMA(loss_list)\n",
    "    plt.plot(loss_list, label = \"loss\")\n",
    "    plt.plot(EMA_loss, label=\"EMA loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title+\"_loss\")\n",
    "    plt.savefig(path_to_save+f\"/{title}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_prediction(title, path_to_save, src, tgt, prediction, sensor_number, index_in, index_tar):\n",
    "\n",
    "    idx_scr = index_in[0, 1:].tolist()\n",
    "    idx_tgt = index_tar[0].tolist()\n",
    "    idx_pred = [i for i in range(idx_scr[0] +1, idx_tgt[-1])] #t2 - t61\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 16})\n",
    "\n",
    "    # connect with last elemenet in src\n",
    "    # tgt = np.append(src[-1], tgt.flatten())\n",
    "    # prediction = np.append(src[-1], prediction.flatten())\n",
    "\n",
    "    # plotting\n",
    "    plt.plot(idx_scr, src, '-', color = 'blue', label = 'Input', linewidth=2)\n",
    "    plt.plot(idx_tgt, tgt, '-', color = 'indigo', label = 'Target', linewidth=2)\n",
    "    plt.plot(idx_pred, prediction,'--', color = 'limegreen', label = 'Forecast', linewidth=2)\n",
    "\n",
    "    #formatting\n",
    "    plt.grid(b=True, which='major', linestyle = 'solid')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(b=True, which='minor', linestyle = 'dashed', alpha=0.5)\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Humidity (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Forecast from Sensor \" + str(sensor_number[0]))\n",
    "\n",
    "    # save\n",
    "    plt.savefig(path_to_save+f\"Prediction_{title}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_training(epoch, path_to_save, src, prediction, sensor_number, index_in, index_tar):\n",
    "\n",
    "    # idx_scr = index_in.tolist()[0]\n",
    "    # idx_tar = index_tar.tolist()[0]\n",
    "    # idx_pred = idx_scr.append(idx_tar.append([idx_tar[-1] + 1]))\n",
    "\n",
    "    idx_scr = [i for i in range(len(src))]\n",
    "    idx_pred = [i for i in range(1, len(prediction)+1)]\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 18})\n",
    "    plt.grid(b=True, which='major', linestyle = '-')\n",
    "    plt.grid(b=True, which='minor', linestyle = '--', alpha=0.5)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    plt.plot(idx_scr, src, 'o-.', color = 'blue', label = 'input sequence', linewidth=1)\n",
    "    plt.plot(idx_pred, prediction, 'o-.', color = 'limegreen', label = 'prediction sequence', linewidth=1)\n",
    "\n",
    "    plt.title(\"Teaching Forcing from Sensor \" + str(sensor_number[0]) + \", Epoch \" + str(epoch))\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Humidity (%)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path_to_save+f\"/Epoch_{str(epoch)}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_3(epoch, path_to_save, src, sampled_src, prediction, sensor_number, index_in, index_tar):\n",
    "\n",
    "    # idx_scr = index_in.tolist()[0]\n",
    "    # idx_tar = index_tar.tolist()[0]\n",
    "    # idx_pred = idx_scr.append(idx_tar.append([idx_tar[-1] + 1]))\n",
    "\n",
    "    idx_scr = [i for i in range(len(src))]\n",
    "    idx_pred = [i for i in range(1, len(prediction)+1)]\n",
    "    idx_sampled_src = [i for i in range(len(sampled_src))]\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 18})\n",
    "    plt.grid(b=True, which='major', linestyle = '-')\n",
    "    plt.grid(b=True, which='minor', linestyle = '--', alpha=0.5)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    ## REMOVE DROPOUT FOR THIS PLOT TO APPEAR AS EXPECTED !! DROPOUT INTERFERES WITH HOW THE SAMPLED SOURCES ARE PLOTTED\n",
    "    plt.plot(idx_sampled_src, sampled_src, 'o-.', color='red', label = 'sampled source', linewidth=1, markersize=10)\n",
    "    plt.plot(idx_scr, src, 'o-.', color = 'blue', label = 'input sequence', linewidth=1)\n",
    "    plt.plot(idx_pred, prediction, 'o-.', color = 'limegreen', label = 'prediction sequence', linewidth=1)\n",
    "    plt.title(\"Teaching Forcing from Sensor \" + str(sensor_number[0]) + \", Epoch \" + str(epoch))\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Humidity (%)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path_to_save+f\"/Epoch_{str(epoch)}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ccd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from DataLoader import SensorDataset\n",
    "import logging\n",
    "import time # debugging\n",
    "from plot import *\n",
    "from helpers import *\n",
    "from joblib import load\n",
    "from icecream import ic\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s %(message)s\", datefmt=\"[%Y-%m-%d %H:%M:%S]\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def transformer(dataloader, EPOCH, frequency, path_to_save_model, path_to_save_loss, path_to_save_predictions, device):\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model = Transformer().double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=200)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    best_model = \"\"\n",
    "    min_train_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCH + 1):\n",
    "\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        ## TRAIN -- TEACHER FORCING\n",
    "        model.train()\n",
    "        for index_in, index_tar, _input, target, sensor_number in dataloader: # for each data set \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Shape of _input : [batch, input_length, feature]\n",
    "            # Desired input for model: [input_length, batch, feature]\n",
    "\n",
    "            src = _input.permute(1,0,2).double().to(device)[:-1,:,:] # torch.Size([24, 1, 7])\n",
    "            target = _input.permute(1,0,2).double().to(device)[1:,:,:] # src shifted by 1.\n",
    "            prediction = model(src, device) # torch.Size([24, 1, 7])\n",
    "            loss = criterion(prediction, target[:,:,0].unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step(loss.detach().item())\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            torch.save(model.state_dict(), path_to_save_model + f\"best_train_{epoch}.pth\")\n",
    "            torch.save(optimizer.state_dict(), path_to_save_model + f\"optimizer_{epoch}.pth\")\n",
    "            min_train_loss = train_loss\n",
    "            best_model = f\"best_train_{epoch}.pth\"\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0: # Plot 1-Step Predictions\n",
    "\n",
    "            logger.info(f\"Epoch: {epoch}, Training loss: {train_loss}\")\n",
    "            scaler = load('scalar_item.joblib')\n",
    "            src_humidity = scaler.inverse_transform(src[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            target_humidity = scaler.inverse_transform(target[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            prediction_humidity = scaler.inverse_transform(prediction[:,:,0].detach().cpu().numpy()) #torch.Size([35, 1, 7])\n",
    "            plot_training(epoch, path_to_save_predictions, src_humidity, prediction_humidity, sensor_number, index_in, index_tar)\n",
    "\n",
    "        train_loss /= len(dataloader)\n",
    "        log_loss(train_loss, path_to_save_loss, train=True)\n",
    "        \n",
    "    plot_loss(path_to_save_loss, train=True)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from DataLoader import SensorDataset\n",
    "import logging\n",
    "import time # debugging\n",
    "from plot import *\n",
    "from helpers import *\n",
    "from joblib import load\n",
    "from icecream import ic\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math, random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s %(message)s\", datefmt=\"[%Y-%m-%d %H:%M:%S]\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def flip_from_probability(p):\n",
    "    return True if random.random() < p else False\n",
    "\n",
    "def transformer(dataloader, EPOCH, k, frequency, path_to_save_model, path_to_save_loss, path_to_save_predictions, device):\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model = Transformer().double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=200)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    best_model = \"\"\n",
    "    min_train_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCH + 1):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        ## TRAIN -- TEACHER FORCING\n",
    "        model.train()\n",
    "        for index_in, index_tar, _input, target, sensor_number in dataloader:\n",
    "        \n",
    "            # Shape of _input : [batch, input_length, feature]\n",
    "            # Desired input for model: [input_length, batch, feature]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            src = _input.permute(1,0,2).double().to(device)[:-1,:,:] # torch.Size([24, 1, 7])\n",
    "            target = _input.permute(1,0,2).double().to(device)[1:,:,:] # src shifted by 1.\n",
    "            sampled_src = src[:1, :, :] #t0 torch.Size([1, 1, 7])\n",
    "\n",
    "            for i in range(len(target)-1):\n",
    "\n",
    "                prediction = model(sampled_src, device) # torch.Size([1xw, 1, 1])\n",
    "                # for p1, p2 in zip(params, model.parameters()):\n",
    "                #     if p1.data.ne(p2.data).sum() > 0:\n",
    "                #         ic(False)\n",
    "                # ic(True)\n",
    "                # ic(i, sampled_src[:,:,0], prediction)\n",
    "                # time.sleep(1)\n",
    "                \"\"\"\n",
    "                # to update model at every step\n",
    "                # loss = criterion(prediction, target[:i+1,:,:1])\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                \"\"\"\n",
    "\n",
    "                if i < 24: # One day, enough data to make inferences about cycles\n",
    "                    prob_true_val = True\n",
    "                else:\n",
    "                    ## coin flip\n",
    "                    v = k/(k+math.exp(epoch/k)) # probability of heads/tails depends on the epoch, evolves with time.\n",
    "                    prob_true_val = flip_from_probability(v) # starts with over 95 % probability of true val for each flip in epoch 0.\n",
    "                    ## if using true value as new value\n",
    "\n",
    "                if prob_true_val: # Using true value as next value\n",
    "                    sampled_src = torch.cat((sampled_src.detach(), src[i+1, :, :].unsqueeze(0).detach()))\n",
    "                else: ## using prediction as new value\n",
    "                    positional_encodings_new_val = src[i+1,:,1:].unsqueeze(0)\n",
    "                    predicted_humidity = torch.cat((prediction[-1,:,:].unsqueeze(0), positional_encodings_new_val), dim=2)\n",
    "                    sampled_src = torch.cat((sampled_src.detach(), predicted_humidity.detach()))\n",
    "            \n",
    "            \"\"\"To update model after each sequence\"\"\"\n",
    "            loss = criterion(target[:-1,:,0].unsqueeze(-1), prediction)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            torch.save(model.state_dict(), path_to_save_model + f\"best_train_{epoch}.pth\")\n",
    "            torch.save(optimizer.state_dict(), path_to_save_model + f\"optimizer_{epoch}.pth\")\n",
    "            min_train_loss = train_loss\n",
    "            best_model = f\"best_train_{epoch}.pth\"\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0: # Plot 1-Step Predictions\n",
    "\n",
    "            logger.info(f\"Epoch: {epoch}, Training loss: {train_loss}\")\n",
    "            scaler = load('scalar_item.joblib')\n",
    "            sampled_src_humidity = scaler.inverse_transform(sampled_src[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            src_humidity = scaler.inverse_transform(src[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            target_humidity = scaler.inverse_transform(target[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            prediction_humidity = scaler.inverse_transform(prediction[:,:,0].detach().cpu().numpy()) #torch.Size([35, 1, 7])\n",
    "            plot_training_3(epoch, path_to_save_predictions, src_humidity, sampled_src_humidity, prediction_humidity, sensor_number, index_in, index_tar)\n",
    "\n",
    "        train_loss /= len(dataloader)\n",
    "        log_loss(train_loss, path_to_save_loss, train=True)\n",
    "        \n",
    "    plot_loss(path_to_save_loss, train=True)\n",
    "    return best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
